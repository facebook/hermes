#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

# -*- coding: utf-8 -*-

# Generates list of unicode ranges belonging to a set of categories
# Usage: genUnicodeTable.py

import datetime
import hashlib
import sys
import urllib.request
from string import Template
from collections import defaultdict


class UnicodeDataFiles:
    # VERSION = "UCD/latest"  # The bleeding edge version of Unicode.
    VERSION = "15.1.0"
    URLS = {
        "UnicodeData.txt": f"http://unicode.org/Public/{VERSION}/ucd/UnicodeData.txt",
        "SpecialCasing.txt": f"http://unicode.org/Public/{VERSION}/ucd/SpecialCasing.txt",
        "CaseFolding.txt": f"http://unicode.org/Public/{VERSION}/ucd/CaseFolding.txt",
        "DerivedGeneralCategory.txt": f"http://unicode.org/Public/{VERSION}/ucd/extracted/DerivedGeneralCategory.txt",
        "Scripts.txt": f"http://unicode.org/Public/{VERSION}/ucd/Scripts.txt",
        "ScriptExtensions.txt": f"http://unicode.org/Public/{VERSION}/ucd/ScriptExtensions.txt",
        "DerivedCoreProperties.txt": f"http://unicode.org/Public/{VERSION}/ucd/DerivedCoreProperties.txt",
        "DerivedNormalizationProps.txt": f"http://unicode.org/Public/{VERSION}/ucd/DerivedNormalizationProps.txt",
        "DerivedBinaryProperties.txt": f"http://unicode.org/Public/{VERSION}/ucd/extracted/DerivedBinaryProperties.txt",
        "PropertyValueAliases.txt": f"http://unicode.org/Public/{VERSION}/ucd/PropertyValueAliases.txt",
        "PropertyAliases.txt": f"http://unicode.org/Public/{VERSION}/ucd/PropertyAliases.txt",
        "PropList.txt": f"http://unicode.org/Public/{VERSION}/ucd/PropList.txt",
        "emoji-data.txt": f"http://unicode.org/Public/{VERSION}/ucd/emoji/emoji-data.txt",
    }
    # Set to True to keep the downloaded files in the local directory.
    KEEP_LOCAL_CACHE = False

    __cache = {}

    @classmethod
    def get(cls, filename):
        """Retrieve a Unicode data file, fetching it if necessary."""
        if filename not in cls.__cache:
            data = cls.__local_or_fetch(cls.URLS[filename], filename)
            cls.__cache[filename] = {
                "sha1": hashlib.sha1(data).hexdigest(),
                "lines": cls.__data_to_lines(data),
            }
        return cls.__cache[filename]

    @classmethod
    def __local_or_fetch(cls, url, filename):
        """Read a local file's contents or fetch them from a URL."""
        try:
            with open(filename, "rb") as f:
                print("Found %s locally!" % filename, file=sys.stderr)
                return f.read()
        except IOError:
            print("Fetching %s..." % url, file=sys.stderr)
            with urllib.request.urlopen(url) as f:
                data = f.read()
                if cls.KEEP_LOCAL_CACHE:
                    print("Caching %s locally..." % filename, file=sys.stderr)
                    with open(filename, "wb") as f:
                        f.write(data)
                return data

    @classmethod
    def __data_to_lines(cls, data):
        return [
            line
            for line in data.decode("utf-8").splitlines()
            if line and not line.startswith("#")
        ]

    @classmethod
    def get_lines(cls, filename):
        return cls.get(filename)["lines"]

    @classmethod
    def get_sha1(cls, filename):
        return cls.get(filename)["sha1"]


# Unicode data field indexes. See UnicodeData.txt.
CODEPOINT_FIELD = 0
GENERAL_CATEGORY_FIELD = 2
UPPERCASE_FIELD = 12
LOWERCASE_FIELD = 13


def print_template(s, **kwargs):
    """Substitute in the keyword arguments to the template string
    (or direct template) s, and print the result, followed by a
    newline.
    """
    text = Template(s).substitute(**kwargs)
    print(text.strip())
    print("")


def print_header():
    print_template(
        """
//
// File generated by genUnicodeTable.py
// using Unicode data files downloaded on ${today}
// for Unicode version ${version}
${sha1s}
// *** DO NOT EDIT BY HAND ***

/// An inclusive range of Unicode characters.
struct UnicodeRange { uint32_t first; uint32_t second; };

/// A UnicodeTransformRange expresses a mapping such as case folding.
/// A character cp is mapped to cp + delta if cp is 0 for the given modulus.
struct UnicodeTransformRange {
    /// The first codepoint of the range.
    unsigned start:24;

    /// The number of characters in the range.
    unsigned count:8;

    /// The signed delta amount.
    int delta:24;

    /// The modulo amount.
    unsigned modulo:8;
};

/// A mapping entry from a name to a canonical name.
struct NameMapEntry {
    std::string_view name;
    std::string_view canonical;
};

/// A mapping entry from a name to an array of UnicodeRange entries.
struct RangeMapEntry {
    std::string_view name;
    llvh::ArrayRef<llvh::ArrayRef<UnicodeRange>> value;
};
""",
        today=str(datetime.date.today()),
        sha1s="\n".join(
            f"// {filename:<30} SHA1: {UnicodeDataFiles.get_sha1(filename)}"
            for filename in UnicodeDataFiles.URLS.keys()
        ),
        version=UnicodeDataFiles.VERSION,
    )


def run_interval(unicode_data_lines, args):
    name = args[0]
    categories = set(args[1:])
    begin = 0
    intervals = []
    last_cp = 0
    openi = False
    for line in unicode_data_lines:
        fields = line.split(";")
        cp_str, category = fields[CODEPOINT_FIELD], fields[GENERAL_CATEGORY_FIELD]
        cp = int(cp_str, 16)
        if category in categories:
            if not openi:
                begin = cp
                openi = True
            else:
                pass  # do nothing we are still in interval
        else:
            if openi:
                intervals.append((begin, last_cp))
                openi = False
            else:
                pass  # keep looking
        last_cp = cp

    if openi:
        intervals.append((begin, last_cp))

    print_template(
        """
// ${args}
// static constexpr uint32_t ${name}_SIZE = $interval_count;
static constexpr UnicodeRange ${name}[] = {
${intervals}
};
    """,
        args=" ".join(args),
        name=name,
        interval_count=len(intervals),
        intervals="\n".join(
            "{" + hex(i[0]) + ", " + hex(i[1]) + "}," for i in intervals
        ),
    )


def print_categories(unicode_data_lines):
    """Output UnicodeRanges for Unicode General Categories."""
    categories = [
        "UNICODE_LETTERS Lu Ll Lt Lm Lo Nl",
        "UNICODE_COMBINING_MARK Mn Mc",
    ]
    for cat in categories:
        run_interval(unicode_data_lines, cat.split())


def get_assigned_codepoints(unicode_data_lines):
    """Gather intervals for all assigned Unicode codepoints."""
    cp_begin = 0
    cp_end = None

    def empty_buf():
        if cp_begin is not None:
            intervals.append((cp_begin, cp_begin if cp_end is None else cp_end))

    intervals = []
    lines = iter(unicode_data_lines)
    last_cp = 0

    while lines:
        line = next(lines, None)
        if line is None:
            break

        fields = split_fields(line)
        cp = int(fields[0], 16)
        # Handle UnicodeData.txt legacy codepoint ranges.
        # <https://www.unicode.org/reports/tr44/#Code_Point_Ranges>
        if fields[1].startswith("<") and fields[1].endswith("First>"):
            empty_buf()
            rng_begin = cp
            rng_end = int(split_fields(next(lines))[0], 16)
            intervals.append((rng_begin, rng_end))
            cp_begin = cp_end = None
        else:
            if cp - last_cp == 1:
                cp_end = cp
            else:
                if cp_begin is not None:
                    empty_buf()
                    cp_end = None
                cp_begin = cp

        last_cp = cp

    return intervals


def split_fields(line):
    return [f.strip() for f in line.split("#")[0].split(";")]


def parse_range(range_str):
    start, end = range_str.split("..") if ".." in range_str else (range_str, range_str)
    return (int(start, 16), int(end, 16))


def parse_codepoint_ranges(lines, pred):
    """
    From an iterable of lines, build a dict mapping canonical property names to
    lists of Unicode codepoint ranges (start, end) for those properties.

    Codepoint ranges will be merged if they are adjacent.
    """
    ranges = defaultdict(list)
    last_name = None
    begin = 0
    last_cp = 0
    openi = False
    for line in lines:
        if not line or line.startswith("#"):
            continue

        fields = split_fields(line)
        codepoint_range = fields[0]
        canonical_name = fields[1]
        if not pred(canonical_name):
            continue

        if last_name is None:
            last_name = canonical_name

        cp_start, cp_end = parse_range(codepoint_range)
        if last_name != canonical_name:
            # We have crossed over a property name boundary.
            if openi:
                ranges[last_name].append((begin, last_cp))
            openi = True
            last_name = canonical_name
            begin = cp_start
        else:
            if openi:
                if cp_start != last_cp + 1:
                    # We have crossed over an interval boundary.
                    ranges[last_name].append((begin, last_cp))
                    begin = cp_start
            else:
                begin = cp_start
                openi = True

        last_cp = cp_end

    if openi:
        ranges[last_name].append((begin, last_cp))

    return ranges


def print_property_ranges(property_ranges, cpp_name, get_args):
    for canonical_name, ranges in property_ranges.items():
        print_template(
            """
// Unicode properties: ${args}
// static constexpr uint32_t ${name}_SIZE = $interval_count;
static constexpr UnicodeRange ${name}[] = {
${intervals}
};
        """,
            args=" ".join(get_args(canonical_name)),
            name=cpp_name(canonical_name),
            interval_count=len(ranges),
            intervals="\n".join(
                f"{{ {hex(start)}, {hex(end)} }}," for (start, end) in ranges
            ),
        )


def print_property_range_mapping(
    var_name,
    canonical_names,
    cpp_name,
    compound_properties={},
    extra_entries={},
    exclude_names=set(),
):
    all_canonical_names = sorted(
        [name for name in canonical_names.keys() if name not in exclude_names]
        + list(extra_entries.keys())
    )

    for canonical_name in all_canonical_names:
        entries = [
            "{{ {inner_name} }},".format(
                # "{{ {inner_name}, std::size({inner_name}) }},".format(
                inner_name=extra_entries.get(inner_name, cpp_name(inner_name))
            )
            for inner_name in sorted(
                compound_properties.get(canonical_name, [canonical_name])
            )
        ]
        print_template(
            """
    static constexpr llvh::ArrayRef<UnicodeRange> ${var_name}_${canonical_name}[] {
    ${entries}
    };
    """,
            var_name=var_name,
            canonical_name=canonical_name,
            entries="\n".join(entries),
        )

    entries = [
        '{{ "{name}", {xxx} }},'.format(
            name=canonical_name, xxx=f"{var_name}_{canonical_name}"
        )
        for canonical_name in all_canonical_names
    ]

    print_template(
        """
static constexpr RangeMapEntry ${var_name}[] = {
${entries}
};
""",
        var_name=var_name,
        entries="\n".join(entries),
    )


def print_property_canonical_name_mapping(var_name, name_mapping):
    print_template(
        """
static constexpr NameMapEntry ${var_name}[] = {
${entries}
};
""",
        var_name=var_name,
        entries="\n".join(
            sorted(
                f'{{ "{other_name}", "{canonical_name}" }},'
                for canonical_name, other_names in name_mapping.items()
                for other_name in other_names
            )
        ),
    )


def parse_property_aliases(lines, get_canonical_name):
    property_aliases = {}
    for line in lines:
        fields = split_fields(line)
        canonical_name = get_canonical_name(fields)
        if canonical_name is not None:
            property_aliases[canonical_name] = fields[1:]
    return property_aliases


def print_general_category_properties(
    property_value_aliases_lines, derived_general_category_lines
):
    """
    Allowed non-binary "General_Category" Unicode character properties.
    """
    property_aliases = parse_property_aliases(
        property_value_aliases_lines,
        get_canonical_name=lambda fields: fields[1] if fields[0] == "gc" else None,
    )

    known_keys = set(property_aliases.keys())
    property_ranges = parse_codepoint_ranges(
        derived_general_category_lines,
        lambda canonical_name: canonical_name in known_keys,
    )

    def cpp_name(name):
        return "UNICODE_PROPERTY_GC_" + property_aliases[name][1].upper()

    print_property_canonical_name_mapping(
        "canonicalPropertyNameMap_GeneralCategory", property_aliases
    )

    print_property_ranges(
        property_ranges,
        cpp_name,
        get_args=lambda canonical_name: property_aliases[canonical_name],
    )

    # These General_Category properties are never directly associated with
    # codepoints, but exist conceptually as unions of other properties.
    #
    # <https://www.unicode.org/reports/tr44/#General_Category_Values>
    COMPOUND_GC_PROPERTIES = {
        "C": ["Cc", "Cf", "Cn", "Co", "Cs"],
        "L": ["Ll", "Lm", "Lo", "Lt", "Lu"],
        "LC": ["Ll", "Lt", "Lu"],
        "M": ["Mc", "Me", "Mn"],
        "N": ["Nd", "Nl", "No"],
        "P": ["Pc", "Pd", "Pe", "Pf", "Pi", "Po", "Ps"],
        "S": ["Sc", "Sk", "Sm", "So"],
        "Z": ["Zl", "Zp", "Zs"],
    }
    print_property_range_mapping(
        "unicodePropertyRangeMap_GeneralCategory",
        property_aliases,
        cpp_name,
        COMPOUND_GC_PROPERTIES,
    )


def print_script_properties(property_value_aliases_lines, scripts_lines):
    """
    Allowed non-binary "Script" Unicode character properties.
    """
    property_aliases = parse_property_aliases(
        property_value_aliases_lines,
        get_canonical_name=lambda fields: fields[2] if fields[0] == "sc" else None,
    )

    # This property is fiction, and is never directly referenced in the
    # codepoint data. Instead, Katakana (Kana) and Hiragana (Hira) are used
    # separately.
    #
    # <https://www.unicode.org/reports/tr44/#Allowed_Changes>
    del property_aliases["Katakana_Or_Hiragana"]

    known_keys = set(property_aliases.keys())
    property_ranges = parse_codepoint_ranges(
        scripts_lines,
        lambda canonical_name: canonical_name in known_keys,
    )

    def cpp_name(canonical_name):
        return "UNICODE_PROPERTY_SC_" + canonical_name.upper()

    print_property_canonical_name_mapping(
        "canonicalPropertyNameMap_Script", property_aliases
    )

    print_property_ranges(
        property_ranges,
        cpp_name,
        get_args=lambda canonical_name: property_aliases[canonical_name],
    )

    print_property_range_mapping(
        "unicodePropertyRangeMap_Script",
        property_aliases,
        cpp_name,
        extra_entries={
            "Unknown": "UNICODE_PROPERTY_GC_UNASSIGNED",
        },
        exclude_names="Unknown",
    )


def print_script_extensions_properties(
    property_value_aliases_lines, script_extensions_lines
):
    """
    Allowed non-binary "Script_Extensions" Unicode character properties.
    """
    property_aliases = parse_property_aliases(
        property_value_aliases_lines,
        get_canonical_name=lambda fields: fields[1] if fields[0] == "sc" else None,
    )
    reverse_property_aliases = parse_property_aliases(
        property_value_aliases_lines,
        get_canonical_name=lambda fields: fields[2] if fields[0] == "sc" else None,
    )

    raw_property_ranges = parse_codepoint_ranges(
        script_extensions_lines, lambda _: True
    )
    property_ranges = defaultdict(list)
    for key, ranges in raw_property_ranges.items():
        for short_key in key.split():
            canonical_name = property_aliases[short_key][1]
            property_ranges[canonical_name].extend(ranges)

    # Trim aliases that are not used.
    for short_key, aliases in property_aliases.items():
        canonical_name = aliases[1]
        if canonical_name in reverse_property_aliases and not property_ranges.get(
            aliases[1]
        ):
            del reverse_property_aliases[canonical_name]

    def cpp_name(canonical_name):
        return "UNICODE_PROPERTY_SCX_" + canonical_name.upper()

    # There is no canonical name mapping for Script_Extensions, instead the one
    # for Script is reused.

    print_property_ranges(
        property_ranges, cpp_name, get_args=lambda name: reverse_property_aliases[name]
    )

    print_property_range_mapping(
        "unicodePropertyRangeMap_ScriptExtensions",
        reverse_property_aliases,
        cpp_name,
    )


def print_binary_properties(
    unicode_data_lines,
    property_aliases_lines,
    prop_list_lines,
    derived_core_properties_lines,
    derived_normalization_props_lines,
    derived_binary_properties_lines,
    emoji_data_lines,
):
    """
    Allowed binary properties explicitly provided by ECMA262.

    <https://tc39.es/ecma262/multipage/text-processing.html#table-binary-unicode-properties>
    """
    property_aliases = {
        canonical_name: []
        for canonical_name in "ASCII ASCII_Hex_Digit Alphabetic Bidi_Control Bidi_Mirrored "
        "Case_Ignorable Cased Changes_When_Casefolded Changes_When_Casemapped "
        "Changes_When_Lowercased Changes_When_NFKC_Casefolded "
        "Changes_When_Titlecased Changes_When_Uppercased Dash "
        "Default_Ignorable_Code_Point Deprecated Diacritic Emoji "
        "Emoji_Component Emoji_Modifier Emoji_Modifier_Base Emoji_Presentation "
        "Extended_Pictographic Extender Grapheme_Base Grapheme_Extend "
        "Hex_Digit IDS_Binary_Operator IDS_Trinary_Operator ID_Continue "
        "ID_Start Ideographic Join_Control Logical_Order_Exception Lowercase "
        "Math Noncharacter_Code_Point Pattern_Syntax Pattern_White_Space "
        "Quotation_Mark Radical Regional_Indicator Sentence_Terminal "
        "Soft_Dotted Terminal_Punctuation Unified_Ideograph Uppercase "
        "Variation_Selector White_Space XID_Continue XID_Start".split()
    }

    for line in property_aliases_lines:
        fields = split_fields(line)
        canonical_name = fields[1]
        if canonical_name in property_aliases:
            assert (
                len(property_aliases[canonical_name]) == 0
            ), "Duplicate canonical name"
            property_aliases[canonical_name] = list(set(fields))

    known_keys = set(property_aliases.keys())
    pred = lambda canonical_name: canonical_name in known_keys
    property_ranges = {
        **parse_codepoint_ranges(prop_list_lines, pred),
        **parse_codepoint_ranges(derived_core_properties_lines, pred),
        **parse_codepoint_ranges(derived_normalization_props_lines, pred),
        **parse_codepoint_ranges(derived_binary_properties_lines, pred),
        **parse_codepoint_ranges(emoji_data_lines, pred),
    }

    # Manually add cases that are not part of the enumerations.
    # <https://unicode.org/reports/tr18/#General_Category_Property>
    property_aliases["ASCII"] = ["ASCII"]
    property_ranges["ASCII"] = [(0x0, 0x7F)]

    property_aliases["Any"] = ["Any"]
    property_ranges["Any"] = [(0x0, 0x10FFFF)]

    property_aliases["Assigned"] = ["Assigned"]
    property_ranges["Assigned"] = get_assigned_codepoints(unicode_data_lines)

    def cpp_name(canonical_name):
        return "UNICODE_PROPERTY_BINARY_" + canonical_name.upper()

    print_property_canonical_name_mapping(
        "canonicalPropertyNameMap_BinaryProperty", property_aliases
    )

    print_property_ranges(
        property_ranges,
        cpp_name,
        get_args=lambda canonical_name: property_aliases[canonical_name],
    )

    print_property_range_mapping(
        "unicodePropertyRangeMap_BinaryProperty", property_aliases, cpp_name
    )


def stride_from(p1, p2):
    return p2[0] - p1[0]


def delta_within(p):
    return p[1] - p[0]


def as_hex(cp):
    return "0x%.4X" % cp


class DeltaMapBlock(object):
    def __init__(self):
        self.pairs = []

    def stride(self):
        return stride_from(self.pairs[0], self.pairs[1])

    def delta(self):
        return delta_within(self.pairs[0])

    def can_append(self, pair):
        if not self.pairs:
            return True
        if pair[0] - self.pairs[0][0] >= 256:
            return False
        if self.delta() != delta_within(pair):
            return False
        return len(self.pairs) < 2 or self.stride() == stride_from(self.pairs[-1], pair)

    @staticmethod
    def append_to_list(blocks, p):
        if not blocks or not blocks[-1].can_append(p):
            blocks.append(DeltaMapBlock())
        blocks[-1].pairs.append(p)

    def output(self):
        pairs = self.pairs
        if not pairs:
            return ""

        first = pairs[0][0]
        last = pairs[-1][0]
        modulo = self.stride() if len(pairs) >= 2 else 1
        delta = self.delta()
        code = Template("{$first, $count, $delta, $modulo}").substitute(
            first=as_hex(first), count=last - first + 1, delta=delta, modulo=modulo
        )
        return code.strip()


class CaseMap(object):
    """Unicode case mapping helper.

    This class holds the list of codepoints, and their uppercase and
    lowercase mappings.

    """

    def __init__(self, unicode_data_lines, special_casing_lines, casefolding_lines):
        """Construct with the lines from UnicodeData and SpecialCasing."""
        self.toupper = {}
        self.tolower = {}
        self.codepoints = []
        for line in unicode_data_lines:
            fields = line.split(";")
            self.__set_casemap(
                fields[CODEPOINT_FIELD],
                upper=fields[UPPERCASE_FIELD],
                lower=fields[LOWERCASE_FIELD],
            )
        self.codepoints.extend(self.toupper.keys())

        # Apply special cases. This is to support ES5.1 Canonicalize, which is
        # cast in terms of toUpperCase(). The desire here is to have a
        # locale-independent result. Thus we ignore SpecialCasing rules that
        # are locale specific. We can also get away with ignoring
        # context-sensitive rules because Canonicalize only considers one
        # character. Thus ignore any rules that have a condition.
        # Format is codepoint, lower, title, upper, condition
        for line in special_casing_lines:
            # Trim comments
            line = line.split("#")[0]
            fields = line.split(";")
            if len(fields) < 5:
                continue
            cps, lower, title, upper, condition = fields[:5]
            # Title is unused
            _ = title  # noqa: F841
            if not condition.strip():
                self.__set_casemap(cps, upper=upper, lower=lower)

        # Characters default to folding to themselves.
        self.folds = {cp: cp for cp in self.codepoints}

        # Parse case folds.
        for line in casefolding_lines:
            fields = line.split("#")[0].split(";")
            if len(fields) != 4:
                continue
            orig, status, folded, _ = map(str.strip, fields)
            # We are only interested in common and simple case foldings.
            if status not in ["C", "S"]:
                continue
            self.folds[int(orig, 16)] = int(folded, 16)

    def __set_casemap(self, cp, upper, lower):
        """Set a case mapping.

        Mark the upper and lower case forms of cp. If a form is empty,
        the character is its own case mapping.
        All parameters are code points encoded via hex into a string.

        """
        # Parse the codepoint from hex.
        cp = int(cp, 16)

        # "The simple uppercase is omitted in the data file if the uppercase
        # is the same as the code point itself."
        # The same is true for the lowercase.
        # Skip eszett or anything else that maps to more than one character.
        self.toupper[cp] = int(upper, 16) if upper and len(upper.split()) == 1 else cp
        self.tolower[cp] = int(lower, 16) if lower and len(lower.split()) == 1 else cp

    def canonicalize(self, ch, unicode):
        """Canonicalize a character per ES9 21.2.2.8.2."""
        if unicode:
            return self.folds[ch]
        else:
            upper_ch = self.toupper[ch]
            # "If u does not consist of a single character, return ch"
            # We only store 1-1 mappings.
            # "If ch's code unit value is greater than or equal to decimal 128
            # and cu's code unit value is less than decimal 128, then return ch"
            # That is, only ASCII may canonicalize to ASCII.
            if upper_ch < 128 and ch >= 128:
                return ch
            return upper_ch


def print_canonicalizations(casemap, unicode):
    blocks = []
    for cp in casemap.codepoints:
        # legacy does not decode surrogate pairs, so we can skip large code points.
        if not unicode and cp > 0xFFFF:
            continue
        canon_cp = casemap.canonicalize(cp, unicode)
        if cp != canon_cp:
            DeltaMapBlock.append_to_list(blocks, (cp, canon_cp))

    print_template(
        """
// static constexpr uint32_t ${name}_SIZE = ${entry_count};
static constexpr UnicodeTransformRange ${name}[] = {
${entry_text}
};
""",
        name="UNICODE_FOLDS" if unicode else "LEGACY_CANONS",
        entry_count=len(blocks),
        entry_text=",\n".join(b.output() for b in blocks),
    )


if __name__ == "__main__":
    print_header()

    print_categories(UnicodeDataFiles.get_lines("UnicodeData.txt"))
    print_general_category_properties(
        UnicodeDataFiles.get_lines("PropertyValueAliases.txt"),
        UnicodeDataFiles.get_lines("DerivedGeneralCategory.txt"),
    )
    print_script_properties(
        UnicodeDataFiles.get_lines("PropertyValueAliases.txt"),
        UnicodeDataFiles.get_lines("Scripts.txt"),
    )
    print_script_extensions_properties(
        UnicodeDataFiles.get_lines("PropertyValueAliases.txt"),
        UnicodeDataFiles.get_lines("ScriptExtensions.txt"),
    )
    print_binary_properties(
        UnicodeDataFiles.get_lines("UnicodeData.txt"),
        UnicodeDataFiles.get_lines("PropertyAliases.txt"),
        UnicodeDataFiles.get_lines("PropList.txt"),
        UnicodeDataFiles.get_lines("DerivedCoreProperties.txt"),
        UnicodeDataFiles.get_lines("DerivedNormalizationProps.txt"),
        UnicodeDataFiles.get_lines("DerivedBinaryProperties.txt"),
        UnicodeDataFiles.get_lines("emoji-data.txt"),
    )

    casemap = CaseMap(
        unicode_data_lines=UnicodeDataFiles.get_lines("UnicodeData.txt"),
        special_casing_lines=UnicodeDataFiles.get_lines("SpecialCasing.txt"),
        casefolding_lines=UnicodeDataFiles.get_lines("CaseFolding.txt"),
    )
    print_canonicalizations(casemap, unicode=True)
    print_canonicalizations(casemap, unicode=False)
