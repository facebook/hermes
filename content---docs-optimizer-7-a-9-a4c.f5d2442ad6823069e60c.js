(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{53:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",function(){return r}),n.d(t,"rightToc",function(){return s}),n.d(t,"default",function(){return h});n(0);var i=n(80);function o(){return(o=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var i in n)Object.prototype.hasOwnProperty.call(n,i)&&(e[i]=n[i])}return e}).apply(this,arguments)}function a(e,t){if(null==e)return{};var n,i,o=function(e,t){if(null==e)return{};var n,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var r={id:"optimizer",title:"Optimizer"},s=[{value:"Design of the Hermes Optimizer",id:"design-of-the-hermes-optimizer",children:[{value:"Introduction",id:"introduction",children:[]},{value:"Key concepts",id:"key-concepts",children:[]}]}],c={rightToc:s},l="wrapper";function h(e){var t=e.components,n=a(e,["components"]);return Object(i.b)(l,o({},c,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("h2",null,Object(i.b)("a",o({parentName:"h2"},{"aria-hidden":!0,className:"anchor",id:"design-of-the-hermes-optimizer"})),Object(i.b)("a",o({parentName:"h2"},{"aria-hidden":!0,className:"hash-link",href:"#design-of-the-hermes-optimizer"}),"#"),"Design of the Hermes Optimizer"),Object(i.b)("h3",null,Object(i.b)("a",o({parentName:"h3"},{"aria-hidden":!0,className:"anchor",id:"introduction"})),Object(i.b)("a",o({parentName:"h3"},{"aria-hidden":!0,className:"hash-link",href:"#introduction"}),"#"),"Introduction"),Object(i.b)("p",null,"This document describes the high-level design of the Hermes optimizer. The\nHermes optimizer transforms the Hermes IR into a more efficient representation\nthat preserves the original semantics of the program. The IR.md document describes\nthe design of the Hermes IR."),Object(i.b)("h3",null,Object(i.b)("a",o({parentName:"h3"},{"aria-hidden":!0,className:"anchor",id:"key-concepts"})),Object(i.b)("a",o({parentName:"h3"},{"aria-hidden":!0,className:"hash-link",href:"#key-concepts"}),"#"),"Key concepts"),Object(i.b)("p",null,"This section describes a few key concepts and ideas:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"The optimizer is responsible for optimizing the IR. IRGen and BytecodeGen\nare not the right place for implementing optimizations. The parts of the\ncompiler that translate from one representation to another are inherently\ncomplex because they require the understanding of the semantics of both\nrepresentations. Moreover, translators are not designed like optimizers.\nThey do not have good access to analysis and do not allow the separation\nof the optimizer from the translation, which makes debugging more\ndifficult.")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"Passes: Optimizations are organized in passes. There are two kinds of\npasses: function passes and module passes. Function passes can modify only\nthe functions that they operate on, while module passes operate on the whole\nmodule.  Function passes are allowed to read the whole module but only\ntouch the current function.")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"Analysis: Analyses are caches in front of a computation of some property.\nFor example, the dominator analysis is a cache that helps reduce compile\ntime by removing the need to recompute the dominator tree for each\nfunction. Analyses are all about caching and invalidating pre-computed\nproperties.")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},'Optimizations do one thing: Optimizations are designed to be simple and\nthis means that they do only one thing. For example, the common\nsubexpression elimination optimization does not delete dead code\n"on the way" just because it can.')),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"Optimizations are predictable: Sometimes there are several legal\nrepresentations of the program, but the optimizer should never\nrandomize the output of the compiler. Randomization of the output\nhappens when the output depends on runtime information such as the order\nof elements in a set or map. Randomizing the output of the compiler\nmakes it very difficult to write tests and reproduce bugs. LLVM has\ndata structures that provide guaranteed order - use them!")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},'Write compile-time efficient algorithms: The compile time of a compiler is a\nvery important metric and we attempt to minimize compile time as much as\npossible. Do not write exponential algorithms (or polynomial algorithm with\na high degree). If you are writing a quadratic algorithm make sure to\nimplement a sliding-window or other techniques that will allows to limit\nthe quadratic search to a small subset of the graph. Always assume that\nthere exist a function with hundreds of consecutive basic blocks or a basic\nblock with thousands of instructions. If you are writing a "solver" then\nyou are probably doing it wrong.')),Object(i.b)("li",{parentName:"ul"},Object(i.b)("p",{parentName:"li"},"There are three kinds of transformations: canonicalization,\nsimplification and lowering. Make sure that you know exactly what kind of\ntransformation you are doing and why. Canonicalizations are transformations that\nexpose opportunities for other transformations.  Re-association (reducing tree\nheight, placing constants on the RHS, etc.) is a canonicalization because it\norganizes things in predictable patterns and makes the life of future\noptimizations simpler by reducing the number of possible inputs. Inlining is\nanother example of effective canonicalization because it exposes opportunities\nfor optimizations in the caller function (by providing more information).\nAnother example is loop rotation, which is a canonical representation of all\nloops.  In canonicalization we strive to clean up the program as much as\npossible and reach a pure representation of the program.  Simplification is what\nwe normally think of as optimizations, like removing redundancy by deleting dead\ncode and optimizing arithmetic, etc.  Canonicalization can allow simplification\nthat can allow more canonicalization.  For example, de-virtualization unblocks\ninlining that may allow some transformations that enable more de-virtualization.\nLowering transformations are the opposite of canonicalization. In Lowering\ntransformations we generate patterns that are closer to the target\nrepresentation. We may not be able to recover from lowering transformations. One\nexample for lowering transformation is loop strength reduction where the\noptimizer transforms loop indices into non-consecutive accesses that fit with\nthe hardware instruction set. Another example is loop versioning where the body\nof the loop is duplicated and versioned multiple times ."))))}h.isMDXComponent=!0}}]);